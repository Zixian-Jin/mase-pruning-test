Script started on 2024-07-29 07:56:56+01:00 [TERM="screen" TTY="/dev/pts/41" COLUMNS="120" LINES="39"]
kcj323@ee-kraken:~/mase-pruning-test\[?2004h[cj323@ee-kraken mase-pruning-test]$ conda activate mase
[?2004lkcj323@ee-kraken:~/mase-pruning-test\[?2004h(mase) [cj323@ee-kraken mase-pruning-test]$ python bert_inference.py 
[?2004lA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.
A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.
Using the latest cached version of the dataset since lansinuote/ChnSentiCorp couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'default' at /mnt/home/cj323/.cache/huggingface/datasets/lansinuote___chn_senti_corp/default/0.0.0/b0c4c119c3fb33b8e735969202ef9ad13d717e5a (last modified on Sun Jul 28 15:13:21 2024).
INFO: loaded downstream model from ./ckpts/downstream_model_unpruned.pth
Sparsity of current module with thres=0.000000 = 0.000000
Sparsity of current module with thres=0.009954 = 0.274238
Sparsity of current module with thres=0.019908 = 0.543498
Sparsity of current module with thres=0.029862 = 0.779060
Sparsity of current module with thres=0.039816 = 0.930440
Sparsity of current module with thres=0.049770 = 0.985277
Sparsity of current module with thres=0.059724 = 0.998203
Sparsity of current module with thres=0.069678 = 0.999844
Sparsity of current module with thres=0.079632 = 0.999959
Sparsity of current module with thres=0.089586 = 0.999993
Top 10% elements in the analysed matrix:
Values= tensor([[0.0441, 0.0320, 0.0311,  ..., 0.0307, 0.0399, 0.0289],
        [0.0455, 0.0444, 0.0421,  ..., 0.0535, 0.0385, 0.0327],
        [0.0419, 0.0415, 0.0312,  ..., 0.0441, 0.0320, 0.0311],
        ...,
        [0.0438, 0.0411, 0.0392,  ..., 0.0340, 0.0520, 0.0326],
        [0.0343, 0.0396, 0.0339,  ..., 0.0543, 0.0455, 0.0329],
        [0.0396, 0.0372, 0.0392,  ..., 0.0324, 0.0359, 0.0320]],
       device='cuda:0')
Indicies= tensor([[  2,  36,  37,  ..., 745, 751, 238],
        [  0,   1,  20,  ..., 731, 748, 637],
        [ 10,  15,  16,  ..., 762, 763, 665],
        ...,
        [  1,  19,  23,  ..., 750, 765, 308],
        [  2,  30,  65,  ..., 760, 761, 283],
        [  7,  18,  20,  ..., 756, 759, 657]], device='cuda:0')
========== Prune after training ===========
INFO: loaded downstream model from ./ckpts/downstream_model_unpruned.pth
Sparsity=0.900000
INFO: Pruning...
INFO: Finished pruning.
INFO: Pruning...
INFO: Finished pruning.
INFO: Pruning...
INFO: Finished pruning.
Before pruning: acc=88.4375. After pruning: acc=89.27083333333333.
========== Prune after training ===========
INFO: loaded downstream model from ./ckpts/downstream_model_unpruned.pth
Sparsity=0.950000
INFO: Pruning...
INFO: Finished pruning.
INFO: Pruning...
INFO: Finished pruning.
INFO: Pruning...
INFO: Finished pruning.
Before pruning: acc=88.4375. After pruning: acc=89.0625.
========== Prune after training ===========
INFO: loaded downstream model from ./ckpts/downstream_model_unpruned.pth
Sparsity=0.980000
INFO: Pruning...
INFO: Finished pruning.
INFO: Pruning...
INFO: Finished pruning.
INFO: Pruning...
INFO: Finished pruning.
Before pruning: acc=88.4375. After pruning: acc=88.85416666666667.
========== Prune after training ===========
INFO: loaded downstream model from ./ckpts/downstream_model_unpruned.pth
Sparsity=1.000000
INFO: Pruning...
INFO: Finished pruning.
INFO: Pruning...
INFO: Finished pruning.
INFO: Pruning...
INFO: Finished pruning.
Before pruning: acc=88.4375. After pruning: acc=89.58333333333333.
kcj323@ee-kraken:~/mase-pruning-test\[?2004h(mase) [cj323@ee-kraken mase-pruning-test]$ nano bert_inference.py 
[?2004l[?2004h(B)0[?1049h[1;39r[m[4l[39;49m[?1h=[?1h=[?25l[39;49m[m[H[J[37;54H[0;7m[ Reading... ][m[37;52H[0;7m[ Read 248 lines ][m[H[0;7m  GNU nano 5.6.1                                      bert_inference.py                                                 [1;119H[m[38d[0;7m^G[m Help[38;16H[0;7m^O[m Write Out   [0;7m^W[m Where Is    [0;7m^K[m Cut[38;61H[0;7m^T[m Execute     [0;7m^C[m Location    [0;7mM-U[m Undo	 [0;7mM-A[m Set Mark[39d[0;7m^X[m Exit[39;16H[0;7m^R[m Read File   [0;7m^\[m Replace     [0;7m^U[m Paste[61G[0;7m^J[m Justify     [0;7m^_[m Go To Line  [0;7mM-E[m Redo	 [0;7mM-6[m Copy[2d[0;1m[36mimport[39m[m torch[3d[0;1m[36mimport[39m[m numpy [0;1m[36mas[39m[m np[4d[0;1m[36mfrom[39m[m datasets [0;1m[36mimport[39m[m load_dataset[5d[0;1m[36mfrom[39m[m transformers [0;1m[36mimport[39m[m BertTokenizer, BertModel, AdamW[6d[0;1m[36mfrom[39m[m torch.utils.data [0;1m[36mimport[39m[m DataLoader, Dataset[8d[0;1m[36mimport[39m[m utils[9d[0;1m[36mimport[39m[m rank_functions[11d[0;1m[36mclass[39m[m BertDataset(Dataset):[12;5H[0;1m[36mdef[34m __init__[39m[m(self, split):[13;9Hself.dataset = load_dataset(path=[0;1m[32m'lansinuote/ChnSentiCorp'[39m[m, split=split)[15;5H[0;1m[36mdef[34m __len__[39m[m(self):[16;9H[0;1m[36mreturn[39m[m len(self.dataset)[18;5H[0;1m[36mdef[34m __getitem__[39m[m(self, i):[19;9Htext = self.dataset[i][[0;1m[32m'text'[39m[m][20;9Hlabel = self.dataset[i][[0;1m[32m'label'[39m[m][21;9H[0;1m[36mreturn[39m[m text, label[24d[42m    [25d[0;1m[31m# print(len(loader))[26d# input_ids.shape, attention_mask.shape, token_type_ids.shape, labels[27d#æ¨¡åž‹è¯•ç®—[28d# out = pretrained(input_ids=input_ids,[29d#            attention_mask=attention_mask,[30d#            token_type_ids=token_type_ids)[31d# out.last_hidden_state.shape[2d[39m[m[34h[?25h[?25l[38;16H         [0;7mM-C[m Case Sens           [0;7mM-B[m Backwards           [0;7m^P[m Older                [0;7m^T[m Go To Line[K[39;2H[0;7mC[m Cancel[16G         [0;7mM-R[m Reg.exp.            [0;7m^R[m Replace              [0;7m^N[m Newer[K[38;110HM[0;7mSearch:                                                                                                                 [37;9H[m[34h[?25h[?25l[0;7mi[m[34h[?25h[?25l[0;7mn[m[34h[?25h[?25l[0;7md[m[34h[?25h[?25l[0;7mi[m[34h[?25h[?25l[0;7mc[m[34h[?25h[?25l[0;7mi[m[34h[?25h[?25l[0;7me[m[34h[?25h[?25l[0;7ms[m[34h[?25h[?25l[K[38;16H[0;7m^O[m Write Out   [0;7m^W[m Where Is    [0;7m^K[m Cut         [0;7m^T[m Execute     [0;7m^C[m Location    [0;7mM-U[m Undo       [0;7mM-A[m Set Mark[39;2H[0;7mX[m Exit  [16G[0;7m^R[m Read File   [0;7m^\[m Replace     [0;7m^U[m Paste       [0;7m^J[m Justify     [0;7m^_[m Go To Line  [0;7mM-E[m Redo	 [0;7mM-6[m Copy[2;8H[1K	torch.save(selected_params, p)[3;8H[1K	print([0;1m[32m'INFO: saved downstream model to %s'[39m[m%p)[4d[K[5d    [0;1m[36mdef[34m load_downstream_model[39m[m(self, p=[0;1m[32m'./ckpts/downstream_model_unpruned.pth'[39m[m):[6;8H[1K	downstream_params = torch.load(p)[K[7;9Hmodel.load_state_dict(downstream_params, strict=[0;1m[35mFalse[39m[m)[8;8H[1K	print([0;1m[32m'INFO: loaded downstream model from %s'[39m[m%p)[9d[K[10;5H[0;1m[36mdef[34m check_sparsity[39m[m(self, module):[11;8H[1K	w = module.weight.data[12;5H   [0;1m[31m # print(w)[39m[m[K[13;9Hthres = 0.05[K[14;9Hmax_val = w.abs().max().item()[15;5H    [0;1m[36mfor[39m[m thres [0;1m[36min[39m[m np.linspace(0.00, max_val, 10):[16;9H    mask = torch.where(torch.abs(w)<thres, 1, 0)[17;13Hprint([0;1m[32m"Sparsity of current module with thres=%f = %f"[39m[m%(thres, torch.sum(mask)/(w.shape[0]*w.shape[1])))[18d[42m        [49m[m[K[19dvalues, [30m[43mindicies[39;49m[m = utils.matrix_profiler(w, rank=0.1, scope=[0;1m[32m'local'[39m[m)[20;9Hprint([0;1m[32m'Top 10% elements in the analysed matrix:'[39m[m)[21;9Hprint([0;1m[32m'Values='[39m[m, values)[22;9Hprint([0;1m[32m'Indicies='[39m[m, indicies)[23d[42m        [24d[49m[m    [0;1m[36mdef[34m simple_prune[39m[m(self, module, thres):[25;8H[1K	print([0;1m[32m'INFO: Pruning...'[39m[m)[26;7H[1K [0;1m[31m # print('Weight before pruning:')[39m[m[K[27;7H[1K [0;1m[31m # print(module.weight.data)[28;8H[39m[m[1K	mask = (torch.abs(module.weight.data) >= thres)[29;8H[1K	module.weight.data *= mask.float() [30;7H[1K [0;1m[31m # print('Weight after pruning:')[39m[m[K[31;7H[1K [0;1m[31m # print(module.weight.data)[32;9H[39m[mprint([0;1m[32m'INFO: Finished pruning.'[39m[m)[34;5H[0;1m[36mdef[34m structured_prune[39m[m(self, module, silent=[0;1m[35mTrue[39m[m):[35;9Hprint([0;1m[32m'INFO: Pruning...'[39m[m)[36;8H[0;1m[31m # module = getattr(self, prune_config['module'])[19;17H[39m[mindicies[20;17H[34h[?25h[?25lM[34h[?25h[?25l[18;9H[34h[?25h[?25l[34h[?25h[?25l[34h[?25h[?25l[34h[?25h[?25l[19d[34h[?25h[?25l [34h[?25h[?25l [34h[?25h[?25l[1;111H[0;7mModified[m[19;7H[0;1m[31m # values, indicies = utils.matrix_profiler(w, rank=0.1, scope='local')[19;9H[39m[m[34h[?25h[?25l  values, indicies = utils.matrix_profiler(w, rank=0.1, scope=[0;1m[32m'local'[39m[m) [8G[34h[?25h[?25l	[34h[?25h[?25l[0;1m[31m #values, indicies = utils.matrix_profiler(w, rank=0.1, scope='local')[19;10H[39m[m[34h[?25h[?25l[0;1m[31m values, indicies = utils.matrix_profiler(w, rank=0.1, scope='local')[19;11H[39m[m[34h[?25h[?25l[20d[34h[?25h[?25l[34h[?25h[?25l	[34h[?25h[?25l[0;1m[31m #print('Top 10% elements in the analysed matrix:')[20;10H[39m[m[34h[?25h[?25l[0;1m[31m print('Top 10% elements in the analysed matrix:')[20;11H[39m[m[34h[?25h[?25l[21d[34h[?25h[?25l[34h[?25h[?25l	[34h[?25h[?25l[0;1m[31m #print('Values=', values)[21;10H[39m[m[34h[?25h[?25l[0;1m[31m print('Values=', values)[21;11H[39m[m[34h[?25h[?25l[22d[34h[?25h[?25l[34h[?25h[?25l	[34h[?25h[?25l[0;1m[31m #print('Indicies=', indicies)[22;10H[39m[m[34h[?25h[?25l[0;1m[31m print('Indicies=', indicies)[22;11H[39m[m[34h[?25h[?25l[23d	[34h[?25h[?25l[24;11H[34h[?25h[?25l[25d[34h[?25h[?25l[26d[34h[?25h[?25l[27d[34h[?25h[?25l[28d[34h[?25h[?25l[29d[34h[?25h[?25l[30d[34h[?25h[?25l[31d[34h[?25h[?25l[32d[34h[?25h[?25l[33d[34h[?25h[?25l[34;11H[34h[?25h[?25l[35d[34h[?25h[?25l[36d[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hdata = module.weight.data[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hsparsity = prune_config[[0;1m[32m'sparsity'[39m[m][36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hmask = rank_functions.block_rank_fn_local(data, prune_config, sparsity, silent=silent)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hmask = mask.to(device)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hmodule.weight.data *= mask[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hprint([0;1m[32m'INFO: Finished pruning.'[39m[m)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5H[0;1m[36mdef[34m bert_attention_prune[39m[m(self, layer_list, weight_list):[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9H[0;1m[32m'''[39m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[0;1m[32m            Ex. layer_list = [0, 1, 3][36;11H[39m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[0;1m[32m            weight_list = ['Q', 'K'][36;11H[39m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[0;1m[32m        '''[39m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hmodules_to_prune = [][36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[42m        [49m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9H[0;1m[36mfor[39m[m layer [0;1m[36min[39m[m layer_list:[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13H[0;1m[36mfor[39m[m weight [0;1m[36min[39m[m weight_list:[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;17H[0;1m[36mif[39m[m weight == [0;1m[32m'Q'[39m[m:[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;21Hmodules_to_prune.append(self.pretrained.encoder.layer._modules[str(layer)].attention.self.query)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;17H[0;1m[36melif[39m[m weight == [0;1m[32m'K'[39m[m:[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;21Hmodules_to_prune.append(self.pretrained.encoder.layer._modules[str(layer)].attention.self.key)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;17H[0;1m[36melif[39m[m weight == [0;1m[32m'V'[39m[m:[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;21Hmodules_to_prune.append(self.pretrained.encoder.layer._modules[str(layer)].attention.self.value)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;17H[0;1m[36melse[39m[m:[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;21Hprint([0;1m[32m'WARNING: Invalid pruned weight has been ignored.'[39m[m)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[42m        [49m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9H[0;1m[36mfor[39m[m module [0;1m[36min[39m[m modules_to_prune:[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13Hself.structured_prune(module)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[0;1m[36mif[39m[m __name__ == [0;1m[32m'__main__'[39m[m:[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hdevice = [0;1m[32m'cuda'[39m[m [0;1m[36mif[39m[m torch.cuda.is_available() [0;1m[36melse[39m[m [0;1m[32m'cpu'[31m # [37m[43mTODO[36;11H[39;49m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hprune_config = {[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13H[0;1m[32m"module"[39m[m: [0;1m[35mNone[39m[m,[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13H[0;1m[32m"scope"[39m[m: [0;1m[32m"local"[39m[m,[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13H[0;1m[32m"block_num"[39m[m: 64,[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13H[0;1m[32m"sparsity"[39m[m: 0.5[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5H}[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hmodel = DownstreamModel()[36;11H[34h[?25h[?25lM[6G[34h[?25h[?25lM	  [34h[?25h[?25l[35;6H[34h[?25h[?25l[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hmodel.to(device)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;4H[0;1m[31m # prune_config['module'] = "fc1"[36;11H[39m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hmodule = model.pretrained.encoder.layer._modules[[0;1m[32m'0'[39m[m].attention.self.query.weight.detach()[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;4H[0;1m[31m # model.downstream_train()[36;11H[39m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[42m    [49m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hmodel.load_downstream_model()[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hmodel.check_sparsity(module=model.fc1)[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hacc_1 = model.downstream_test()[36;11H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[42m    [49m[m[34h[?25h[?25lM	1 [34h[?25h[?25lM[34h[?25h[?25l[35d[34h[?25h[?25l[36;5H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hlayers_to_prune = [3][0;1m[31m # list(range(12))[36;11H[39m[m[34h[?25h[?25lM[5G[34h[?25h[?25lM	1 [34h[?25h[?25lM[34h[?25h[?25l[34h[?25h[?25l	[34h[?25h[?25l[34h[?25h[?25l[34h[?25h[?25l[34h[?25h[?25l[34h[?25h[?25l[0;1m[31m #model.check_sparsity(module=model.fc1)[6G[39m[m[34h[?25h[?25l[0;1m[31m model.check_sparsity(module=model.fc1)[7G[39m[m[34h[?25h[?25l[34d[34h[?25h[?25l[35d[34h[?25h[?25l[36dla[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5Hweights_to_prune = [[0;1m[32m'Q'[39m[m, [0;1m[32m'K'[39m[m, [0;1m[32m'V'[39m[m][7G[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[42m    [49m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;4H[0;1m[31m # for i in [0.7, 0.8, 0.9, 0.92, 0.95, 0.98, 1.00]:[7G[39m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5H[0;1m[36mfor[39m[m i [0;1m[36min[39m[m [0.9, 0.95, 0.98, 1.00]:[7G[34h[?25h[?25lM[34h[?25h[?25lM[34h[?25h[?25l[35d[K[36;4H[0;1m[31m # for i in [0.7, 0.8, 0.9, 0.92, 0.95, 0.98, 1.00]:M[39m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l    p[34h[?25h[?25lr[34h[?25h[?25li[34h[?25h[?25l[0;1m[36mprint[39m[m[34h[?25h[?25l[5Gprint([34h[?25h[?25l'[34h[?25h[?25lP[34h[?25h[?25lr[34h[?25h[?25lu[34h[?25h[?25li[34h[?25h[?25ln[34h[?25h[?25lg[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l C[34h[?25h[?25lo[34h[?25h[?25ln[34h[?25h[?25lf[34h[?25h[?25li[34h[?25h[?25lg[34h[?25h[?25lu[34h[?25h[?25lr[34h[?25h[?25la[34h[?25h[?25lit[34h[?25h[?25lo[34h[?25h[?25ln[34h[?25h[?25ls[34h[?25h[?25l:[34h[?25h[?25l[35;11H[0;1m[32m'Pruing Configuraitons:'[39m[m[34h[?25h[?25l)[34h[?25h[?25l[36d[K[34h[?25h[?25lp[34h[?25h[?25lr[34h[?25h[?25li[34h[?25h[?25ln[34h[?25h[?25l[0;1m[36mprint[39m[m[34h[?25h[?25lprint([34h[?25h[?25l[0;1m[36mprint[39m[m [34h[?25h[?25lprin [34h[?25h[?25l [34h[?25h[?25l [34h[?25h[?25l [34h[?25h[?25l[K[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l    p[34h[?25h[?25lr[34h[?25h[?25li[34h[?25h[?25ln[34h[?25h[?25l[0;1m[36mprint[39m[m[34h[?25h[?25l[5Gprint([34h[?25h[?25lp[34h[?25h[?25lr[34h[?25h[?25lu[34h[?25h[?25ln[34h[?25h[?25l_[34h[?25h[?25l [34h[?25h[?25le[34h[?25h[?25l_[34h[?25h[?25lc[34h[?25h[?25lo[34h[?25h[?25ln[34h[?25h[?25lf[34h[?25h[?25li[34h[?25h[?25lg[34h[?25h[?25l)[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l    int[34h[?25h[?25l([34h[?25h[?25l [34h[?25h[?25l[0;1m[36min[39m[m [34h[?25h[?25li [34h[?25h[?25l[42m    [49m[m [34h[?25h[?25l    p[34h[?25h[?25lr[34h[?25h[?25li[34h[?25h[?25ln[34h[?25h[?25l[0;1m[36mprint[39m[m[34h[?25h[?25l[5Gprint([34h[?25h[?25lM[34h[?25h[?25l'prune_config)[35;12H[34h[?25h[?25lBprune_config)[35;13H[34h[?25h[?25lpprune_config)[35;14H[34h[?25h[?25l[1P[34h[?25h[?25l[1P[34h[?25h[?25lBprune_config)[35;13H[34h[?25h[?25llprune_config)[35;14H[34h[?25h[?25loprune_config)[35;15H[34h[?25h[?25lcprune_config)[35;16H[34h[?25h[?25lkprune_config)[35;17H[34h[?25h[?25l_prune_config)[35;18H[34h[?25h[?25lnprune_config)[35;19H[34h[?25h[?25luprune_config)[35;20H[34h[?25h[?25lmprune_config)[35;21H[34h[?25h[?25l=prune_config)[35;22H[34h[?25h[?25l[35;11H[0;1m[32m'Block_num='[39m[mprune_config)[35;23H[34h[?25h[?25l,prune_config)[35;24H[34h[?25h[?25l prune_config)[35;25H[34h[?25h[?25lp[34h[?25h[?25lr[34h[?25h[?25lu[34h[?25h[?25ln[34h[?25h[?25le[34h[?25h[?25l_[34h[?25h[?25lc[34h[?25h[?25l	[34h[?25h[?25ln[34h[?25h[?25lf[34h[?25h[?25li[34h[?25h[?25lg[34h[?25h[?25l[)[34h[?25h[?25l')[34h[?25h[?25lb)[34h[?25h[?25ll)[Z[34h[?25h[?25lo)[34h[?25h[?25lc)[34h[?25h[?25lk)[34h[?25h[?25l_)[34h[?25h[?25ln)[34h[?25h[?25lu)[34h[?25h[?25lm)[34h[?25h[?25l])[Z[34h[?25h[?25l)[K[34h[?25h[?25l[35;38H[0;1m[32m'block_num'[39m[m)[Z[34h[?25h[?25l])[34h[?25h[?25l)[34h[?25h[?25l[36d[34h[?25h[?25l [34h[?25h[?25l [34h[?25h[?25l [34h[?25h[?25l [34h[?25h[?25lp[34h[?25h[?25lri[34h[?25h[?25l	[34h[?25h[?25lt[34h[?25h[?25l([34h[?25h[?25l7[2;37r8[37d
[1;39r[36;4H[0;1m[31m # for i in [0.7, 0.8, 0.9, 0.92, 0.95, 0.98, 1.00]:[39m[m[34h[?25h[?25l [34h[?25h[?25l[34h[?25h[?25lM[34h[?25h[?25l [34h[?25h[?25l [34h[?25h[?25l [34h[?25h[?25l [34h[?25h[?25lp[34h[?25h[?25lr[34h[?25h[?25li[34h[?25h[?25l	[34h[?25h[?25lt[34h[?25h[?25l([34h[?25h[?25l'[34h[?25h[?25lL[34h[?25h[?25la[34h[?25h[?25ly[34h[?25h[?25le[34h[?25h[?25lr[34h[?25h[?25ls[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l t[34h[?25h[?25lo[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l p[34h[?25h[?25lr[34h[?25h[?25lu[34h[?25h[?25ln[34h[?25h[?25le[34h[?25h[?25l=[34h[?25h[?25l[35;11H[0;1m[32m'Layers to prune='[39m[m[34h[?25h[?25l,[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l l[34h[?25h[?25la[34h[?25h[?25ly[34h[?25h[?25le[34h[?25h[?25lr[34h[?25h[?25ls[34h[?25h[?25l_[34h[?25h[?25lt[34h[?25h[?25lo[34h[?25h[?25l_[34h[?25h[?25lp[34h[?25h[?25lr[34h[?25h[?25lu[34h[?25h[?25ln[34h[?25h[?25le[34h[?25h[?25l)[34h[?25h[?25l)[34h[?25h[?25l [34h[?25h[?25l[36d[K[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l    p[34h[?25h[?25lr[34h[?25h[?25li[34h[?25h[?25ln[34h[?25h[?25l[0;1m[36mprint[39m[m[34h[?25h[?25l[5Gprint([34h[?25h[?25l'[34h[?25h[?25lW[34h[?25h[?25le[34h[?25h[?25li[34h[?25h[?25lg[34h[?25h[?25lh[34h[?25h[?25lt[34h[?25h[?25ls[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l t[34h[?25h[?25lo[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l p[34h[?25h[?25lr[34h[?25h[?25lu[34h[?25h[?25ln[34h[?25h[?25le[34h[?25h[?25l=[34h[?25h[?25l[36;11H[0;1m[32m'Weights to prune='[39m[m[34h[?25h[?25l,[34h[?25h[?25l[42m [49m[m[34h[?25h[?25l w[34h[?25h[?25le[34h[?25h[?25li[34h[?25h[?25lg[34h[?25h[?25lh[34h[?25h[?25lt[34h[?25h[?25ls[34h[?25h[?25l_[34h[?25h[?25lt[34h[?25h[?25lo[34h[?25h[?25l_[34h[?25h[?25lp[34h[?25h[?25lr[34h[?25h[?25lu[34h[?25h[?25ln[34h[?25h[?25le[34h[?25h[?25l)[34h[?25h[?25lM[34h[?25h[?25lM	[34h[?25h[?25l[33;36H[34h[?25h[?25lM[5G[34h[?25h[?25l[31;39H[34h[?25h[?25lM[44G[34h[?25h[?25lM[5G[34h[?25h[?25l[28;36H[34h[?25h[?25l[27;45H[34h[?25h[?25l[26;34H[34h[?25h[?25l[24;31H[34h[?25h[?25l[23;49H[34h[?25h[?25l[21;21H[34h[?25h[?25l[20;30H[34h[?25h[?25lMM[34h[?25h[?25lM,[34h[?25h[?25lM,[34h[?25h[?25lM[34h[?25h[?25l[14;21H[34h[?25h[?25l[15;28H[34h[?25h[?25l[16;30H[34h[?25h[?25l[17d[34h[?25h[?25l[34h[?25h[?25l[18d[34h[?25h[?25l[19;6H[34h[?25h[?25l[20;28H[34h[?25h[?25l[21;21H[34h[?25h[?25l[22;28H[34h[?25h[?25l[23d[34h[?25h[?25l[24d[34h[?25h[?25l[25;5H[34h[?25h[?25l[26;28H[34h[?25h[?25l[27d[34h[?25h[?25l[28d[34h[?25h[?25l[29;5H[34h[?25h[?25l[30;28H[34h[?25h[?25l[31d[34h[?25h[?25l[32;5H[34h[?25h[?25l[33;28H[34h[?25h[?25l[34d[34h[?25h[?25l[35d[34h[?25h[?25l[36d[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;4H[0;1m[31m # for i in [0.7, 0.8, 0.9, 0.92, 0.95, 0.98, 1.00]:[36;28H[39m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;5H[0;1m[36mfor[39m[m i [0;1m[36min[39m[m [0.9, 0.95, 0.98, 1.00]:[36;28H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hprint([0;1m[32m'========== Prune after training ==========='[39m[m)[36;28H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hmodel.load_downstream_model()[36;28H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hprint([0;1m[32m"Sparsity=%f"[39m[m%i)[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hprune_config = {[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13H[0;1m[32m"module"[39m[m: [0;1m[32m'fc1'[39m[m,[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13H[0;1m[32m"scope"[39m[m: [0;1m[32m"local"[39m[m,[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13H[0;1m[32m"block_num"[39m[m: 64,[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;13H[0;1m[32m"sparsity"[39m[m: i[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9H}[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;8H[0;1m[31m # model.structured_prune(module=model.fc1)[36;28H[39m[m[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hmodel.bert_attention_prune(layers_to_prune, weights_to_prune)[36;28H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hacc_2 = model.downstream_test()[36;28H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;9Hprint(f[0;1m[32m"Before pruning: acc={acc_1}. After pruning: acc={acc_2}."[39m[m)[36;28H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[34h[?25h[?25l7[2;37r8[37d
[1;39r[36;1H[34h[?25h[?25l[37d[0;7mSave modified buffer?                                                                                                   [38;1H Y[m Yes[K[39d[0;7m N[m No  [39;16H [0;7m^C[m Cancel[K[37;23H[34h[?25h[?25l[34h[?25h[?25l[38d[0;7m^G[m Help[38;31H[0;7mM-D[m DOS Format[38;61H[0;7mM-A[m Append[38;91H[0;7mM-B[m Backup File[39d[0;7m^C[m Cancel	              [0;7mM-M[m Mac Format[39;61H[0;7mM-P[m Prepend[39;91H[0;7m^T[m BrowseMM[0;7mFile Name to Write: bert_inference.py[m[34h[?25h[?25l[37;53H[1K [0;7m[ Writing... ][m[K[1;111H[0;7m        [m[37;51H[0;7m[ Wrote 252 lines ][m[J[39d[34h[?25h[39;1H[?1049l[?1l>[?2004lkcj323@ee-kraken:~/mase-pruning-test\[?2004h(mase) [cj323@ee-kraken mase-pruning-test]$ 
[?2004lkcj323@ee-kraken:~/mase-pruning-test\[?2004h(mase) [cj323@ee-kraken mase-pruning-test]$ 
[?2004lkcj323@ee-kraken:~/mase-pruning-test\[?2004h(mase) [cj323@ee-kraken mase-pruning-test]$ 
[?2004lkcj323@ee-kraken:~/mase-pruning-test\[?2004h(mase) [cj323@ee-kraken mase-pruning-test]$ 
[?2004lkcj323@ee-kraken:~/mase-pruning-test\[?2004h(mase) [cj323@ee-kraken mase-pruning-test]$ 
[?2004lkcj323@ee-kraken:~/mase-pruning-test\[?2004h(mase) [cj323@ee-kraken mase-pruning-test]$ scr[K[K[Kexit
[?2004lexit

Script done on 2024-07-29 08:07:31+01:00 [COMMAND_EXIT_CODE="0"]
